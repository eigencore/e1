<h1 align="center">
  e1 by EigenCore
</h1>

<p align="center">
  <a href="https://github.com/eigencore/e1"><img src="https://img.shields.io/badge/1B_parÃ¡metros-FF6B35?style=flat-square" /></a>
  <a href="https://github.com/eigencore/e1"><img src="https://img.shields.io/badge/PyTorch_puro-EE4C2C?style=flat-square&logo=pytorch&logoColor=white" /></a>
  <a href="https://github.com/eigencore/e1"><img src="https://img.shields.io/badge/LLaMA--3_style-6C63FF?style=flat-square" /></a>
  <a href="https://github.com/eigencore/e1"><img src="https://img.shields.io/badge/100B_tokens-8B5CF6?style=flat-square" /></a>
  <a href="https://github.com/eigencore/e1"><img src="https://img.shields.io/badge/EspaÃ±ol_primero-F59E0B?style=flat-square" /></a>
  <a href="https://github.com/eigencore/e1"><img src="https://img.shields.io/badge/Apache_2.0-22C55E?style=flat-square" /></a>
</p>

---

<h3 align="center">El primer LLM moderno construido desde cero en LATAM</h3>

<p align="center">
  <a href="#">DocumentaciÃ³n</a> Â·
  <a href="#">Paper tÃ©cnico</a> Â·
  <a href="https://x.com/MaxGalindo5">Sigue el build en X</a> Â·
  <a href="#">EigenCore</a>
</p>

---

La mayorÃ­a de la gente usa modelos de lenguaje. Nosotros construimos uno.

**e1** es el primer modelo de lenguaje a gran escala de EigenCore â€” 1 billÃ³n de parÃ¡metros, arquitectura moderna, entrenado completamente en espaÃ±ol, escrito lÃ­nea por lÃ­nea en PyTorch puro. Sin HuggingFace Trainer. Sin Lightning. Sin DeepSpeed. Cada forward pass, cada gradiente, cada operaciÃ³n que ves aquÃ­ fue escrita a mano y estÃ¡ disponible para que la leas, la entiendas y aprendas de ella.

> Esto no es un fine-tune. Esto no es un wrapper. Esto es el modelo real.

---

## Â¿QuÃ© hay adentro?

| MÃ³dulo | DescripciÃ³n |
|---|---|
| ğŸ”„ **RoPE** | Rotary Position Embeddings implementado desde cero |
| âš¡ **FlashAttention-2** | AtenciÃ³n eficiente vÃ­a `scaled_dot_product_attention` |
| ğŸ¯ **GQA** | Grouped Query Attention â€” 16 Q heads, 8 KV heads |
| ğŸ§± **SwiGLU** | ActivaciÃ³n moderna del FFN |
| ğŸ“ **RMSNorm** | NormalizaciÃ³n sin bias, mÃ¡s rÃ¡pida que LayerNorm |
| ğŸ”€ **FSDP2** | Multi-GPU nativo de PyTorch, sin DeepSpeed |
| ğŸ’¾ **MMap Dataset** | Pipeline para 100B+ tokens sin explotar la RAM |
| ğŸ“ **SFT + DPO** | Post-training completo implementado desde cero |

---

## Arquitectura

```
e1-1B
â”œâ”€â”€ ParÃ¡metros:      1,000,000,000
â”œâ”€â”€ Capas:           22
â”œâ”€â”€ d_model:         2048
â”œâ”€â”€ Attention heads: 16Q / 8KV (GQA)
â”œâ”€â”€ FFN hidden:      8192 (SwiGLU)
â”œâ”€â”€ Context length:  4096 tokens
â”œâ”€â”€ Vocabulario:     32,000 (espaÃ±ol)
â””â”€â”€ Entrenado en:    100B tokens
```

---

## Por quÃ© construimos nuestro propio framework

Porque la mejor forma de entender algo es construirlo tÃº mismo.

Cada abstracciÃ³n que HuggingFace te oculta, aquÃ­ la exponemos. Cada truco enterrado dentro de DeepSpeed, aquÃ­ lo implementamos y explicamos. Si alguna vez usaste un modelo de lenguaje y te preguntaste quÃ© estÃ¡ pasando realmente por debajo â€” este repositorio es para ti.

El training loop completo son **~2,000 lÃ­neas de PyTorch puro**. Nada mÃ¡s.

---

## Inicio rÃ¡pido

```bash
git clone https://github.com/eigencore/e1
cd e1
```

## Estado del proyecto

| Fase | Estado |
|---|---|
| Arquitectura base | ğŸ”„ En progreso |
| Training loop | â³ PrÃ³ximamente |
| Data pipeline | â³ PrÃ³ximamente |
| Primer training run | â³ PrÃ³ximamente |
| Post-training (SFT) | â³ PrÃ³ximamente |
| Post-training (DPO) | â³ PrÃ³ximamente |

---

<p align="center">
  Construido en pÃºblico desde LATAM ğŸŒ por <a href="https://github.com/eigencore">EigenCore</a>
</p>
